{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6001 5.1.Lab\n",
    "\n",
    "## QR Factorization\n",
    "\n",
    "So far you have already learned the $LU$ decomposition/factorization, which was by far the most common method of obtaining a solution to a linear system for some time, until the *Francis method* of $QR$ decomposition came about in the 60's. The Francis method $QR$ decomposition not only gives the eigenvectors to a matrix, but the eigenvalues and the solution to the system as well. $LU$ is still the most common method for decomposing small matrices as it's somewhat faster, but less stable. \n",
    "\n",
    "Shortly after this, methods involving *Givens rotations* and *Householder reflections* came about (better stability). The latter method is the best for larger matrices. For very large matrices only approximate methods are possible even today due to the high order ( $O(N^{3})$ ) of all of these algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts\n",
    "\n",
    "It is a **theorem** that *any nonsingular (invertible) matrix can be factored into a product of two matrices*: A matrix $Q$ of orthogonal vectors (representing an image-preserving map), and an upper-right triangular matrix $R$ much like the $U$ matrix of the $LU$ factorization. From this we may obtain a unique solution to the system.\n",
    "\n",
    "#### Factoring noninvertible matrices (?!?!) \n",
    "\n",
    "So far all your effort (and for the rest of this class) has been bent on factoring invertible or nonsingular matrices. It may seem perhaps that there are at least as many examples of noninvertible or singular matrices that might need to be factored. \n",
    "\n",
    "In this case, matrix factorization proceeds by a **least-squares** algorithm. This will be covered somewhat later. Least-squares methods are what are used in industry to provide matrix factorizations at scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of the QR factorization\n",
    "\n",
    "Let us describe a square **matrix, A**:\n",
    "\n",
    "$$ {\\bf{A}} = \\{\\vec{a_{,1}}, \\vec{a_{,2}}, \\vec{a_{,3}}, \\cdots, \\vec{a_{,n}}\\}$$\n",
    "\n",
    "Now let us apply the Gram-Schmidt process to the columns of ${\\bf{A}}$, to obtain a new set of orthonormal column vectors. These vectors describe an orthornormal projection of the image space of the original ${\\bf{A}}$:\n",
    "\n",
    "$$q_1 = \\dfrac{u_{,1}}{\\|u_{,1}\\|}; u_{,1} = a_{,1}$$\n",
    "\n",
    "$$q_2 =  \\dfrac{u_{,2}}{\\|u_{,2}\\|}; u_{,2} = a_{,2}-(a_{,2} \\cdot u_{,1}) u_{,1}$$\n",
    "\n",
    "$$q_3 = \\dfrac{u_{,3}}{\\|u_{,3}\\|}; u_{,3} = a_{,3}-(a_{,3} \\cdot u_{,2}) u_{,2} -(a_{,3} \\cdot u_{,1}) u_{,1}$$\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "$$q_{k+1} = \\dfrac{u_{,k+1}}{\\|u_{,k+1}\\|}; u_{,k+1} = a_{,k+1} - \\sum_{n=1}^{k}(a_{,k+1} \\cdot u_{,n})u_{,n}$$\n",
    "\n",
    "And hence form the $N$ columns of the matrix ${\\bf{Q}}$:\n",
    "\n",
    "$${\\bf{Q}} = \\begin{bmatrix} \\vec{q_1} & \\vec{q_2} & \\cdots & \\vec{q_{N}}\\end{bmatrix}$$\n",
    "\n",
    "Now it so turns out that this happens to be a decomposition of ${\\bf{A}}$ if we realize that we can dot the rows of ${\\bf{Q}}$ with another matrix if we want to simply return the elements of ${\\bf{A}}$:\n",
    "\n",
    "$$\\begin{bmatrix} a_{1,1} & a_{1,2} & \\cdots & a_{1,N}\\\\ a_{2,1} & a_{2,2} & \\cdots & a_{2,N}\\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ a_{N,1} & a_{M,2} & \\cdots & a_{N,N}\n",
    "\\end{bmatrix} = \\begin{bmatrix} q_{1,1} & q_{1,2} & \\cdots & q_{1,N}\\\\ q_{2,1} & q_{2,2} & \\cdots & q_{2,N}\\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ q_{M,1} & q_{M,2} & \\cdots & q_{M,N}\n",
    "\\end{bmatrix}\\begin{bmatrix} q_{1,1}a_{1,1} & q_{1,2}a_{1,2} & \\cdots & q_{1,N}a_{1,N}\\\\ 0 & q_{2,2}a_{2,2} & \\cdots & q_{2,N}a_{2,N}\\\\ 0 & 0 & \\cdots & \\cdots \\\\ 0 & 0 & 0 & q_{N,N}a_{N,N}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$${\\bf{Q}} = \\begin{bmatrix} | & | & |\\\\\n",
    "                             q_{,1} & q_{,2} & q_{,3} \\\\  \n",
    "                             | & | & | \\end{bmatrix}$$\n",
    "\n",
    "$${\\bf{R}} = \\begin{bmatrix} a_{,1} \\cdot q_{,1} & a_{,2} \\cdot q_{,1} & a_{,3} \\cdot q_{,1} \\\\\n",
    "                             0 & a_{,2} \\cdot q_{,2} & a_{,3} \\cdot q_{,2}  \\\\  \n",
    "                             0 & 0 & a_{,3} \\cdot q_{,3} \\end{bmatrix}$$\n",
    "\n",
    "This is because each element of ${\\bf{Q}}$ contains the magnitude of the dot product, thus the inner product of each row by column results in the appropriate element of ${\\bf{A}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "Compute the QR factorization for the matrix using Gramm-Schmidt:\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$ \\vec{u^{(1)}} = \\vec{a^{(1)}}$$\n",
    "$$\\vec{q_{,1}} =\\dfrac{u_{,1}}{\\|u_{,1}\\|} = \\dfrac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}$$\n",
    "\n",
    "$$ \\vec{u_{,2}} = a_{,2}-(a_{,2} \\cdot u_{,1}) u_{,1} = \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} - \\dfrac{1}{\\sqrt{2}}\\begin{bmatrix}\\dfrac{1}{\\sqrt{2}}\\\\\\dfrac{1}{\\sqrt{2}}\\\\0\\end{bmatrix} =  \\begin{bmatrix}\\dfrac{1}{2}\\\\-\\dfrac{1}{2}\\\\1\\end{bmatrix}$$\n",
    "\n",
    "$$\\vec{q_{,2}} = \\dfrac{u_{,2}}{\\|u_{,2}\\|} = \\begin{bmatrix}\\dfrac{1}{\\sqrt{6}}\\\\-\\dfrac{1}{\\sqrt{6}}\\\\ \\dfrac{2}{\\sqrt{6}}\\end{bmatrix}$$\n",
    "\n",
    "$$ \\vec{u_{,3}} = a_{,3}-(a_{,3} \\cdot u_{,2}) u_{,2}-(a_{,3} \\cdot u_{,1}) u_{,1} $$\n",
    "\n",
    "$$ \\vec{u_{,3}} = \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix} -\\dfrac{1}{\\sqrt{6}}\\begin{bmatrix}\\dfrac{1}{\\sqrt{6}}\\\\-\\dfrac{1}{\\sqrt{6}}\\\\ \\dfrac{2}{\\sqrt{6}}\\end{bmatrix} - \\dfrac{1}{\\sqrt{2}}\\begin{bmatrix}\\dfrac{1}{\\sqrt{2}}\\\\\\dfrac{1}{\\sqrt{2}}\\\\0\\end{bmatrix} = \\begin{bmatrix}-\\dfrac{1}{\\sqrt{3}}\\\\\\dfrac{1}{\\sqrt{3}}\\\\ \\dfrac{1}{\\sqrt{3}}\\end{bmatrix}$$\n",
    "\n",
    "Thus we have a full basis for the image space of $\\bf{Q}$:\n",
    "\n",
    "$$\\left\\{ \\begin{bmatrix}\\dfrac{1}{\\sqrt{2}} \\\\ \\dfrac{1}{\\sqrt{2}}\\\\0 \\end{bmatrix}, \\begin{bmatrix}\\dfrac{1}{\\sqrt{6}}\\\\-\\dfrac{1}{\\sqrt{6}}\\\\ \\dfrac{2}{\\sqrt{6}}\\end{bmatrix}, \\begin{bmatrix}-\\dfrac{1}{\\sqrt{3}}\\\\\\dfrac{1}{\\sqrt{3}}\\\\ \\dfrac{1}{\\sqrt{3}}\\end{bmatrix}  \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can construct Q confidently:\n",
    "\n",
    "$${\\bf{Q}} = \\begin{bmatrix}\\dfrac{1}{\\sqrt{2}} & \\dfrac{1}{\\sqrt{6}} & -\\dfrac{1}{\\sqrt{3}} \\\\ \\dfrac{1}{\\sqrt{2}} & -\\dfrac{1}{\\sqrt{6}} & \\dfrac{1}{\\sqrt{3}} \\\\ 0 & \\dfrac{2}{\\sqrt{6}} & \\dfrac{1}{\\sqrt{3}} \\end{bmatrix}$$\n",
    "\n",
    "And R follows simply with elements that are the proscribed dot products:\n",
    "\n",
    "$${\\bf{R}} = \\begin{bmatrix}\\dfrac{\\sqrt{2}}{2} & \\dfrac{1}{\\sqrt{3}} & \\dfrac{1}{\\sqrt{2}} \\\\ 0 & \\dfrac{3}{\\sqrt{6}} & \\dfrac{1}{\\sqrt{6}} \\\\ 0 & 0 & \\dfrac{2}{\\sqrt{3}} \\end{bmatrix}$$\n",
    "\n",
    "### QUIZ:\n",
    "\n",
    "Satisfy yourself that this is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [1 0 1]\n",
      " [0 1 1]]\n",
      "[[  4.08248290e-01   7.07106781e-01   5.77350269e-01]\n",
      " [ -8.16496581e-01   1.19612948e-17   5.77350269e-01]\n",
      " [  4.08248290e-01  -7.07106781e-01   5.77350269e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "from math import copysign, hypot\n",
    "\n",
    "A = np.array([[1, 1, 0 ],[1, 0, 1],[0, 1, 1]])\n",
    "\n",
    "print(A)\n",
    "\n",
    "(l, Q) = LA.eig(A)\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: \n",
    "\n",
    "Use the Gramm-Schmidt algorithm (by hand) to produce the QR factorization of the following matrix:\n",
    "\n",
    "$$ A = \\begin{bmatrix} -2 & 0 & 1\\\\ 1 & -2 & 1 \\\\ 1 & -1 & 0\\end{bmatrix}$$\n",
    "\n",
    "Show most of your steps. You may use the computer to check your steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u${1} = [-2,1,1]^T$, but we need to normalize the vector: the norm of [-2,1,1]$^T$ is $\\sqrt6$, so \n",
    "\n",
    "u${1} = \\frac{1}{\\sqrt6} * [-2,1,1]^T$\n",
    "\n",
    "u${2} = [0,-2,-1]^T - ([0,-2,-1]^T * \\frac{1}{\\sqrt6} * [-2,1,1]^T) * \\frac{1}{\\sqrt6} * [-2,1,1]^T = [0,-2,-1]^T - \\frac{1}{3} * [-2,1,1]^T = [0,-2,-1]^T + [-1,1/2,1/2]^T = [-1,-3/2,-1/2]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33333333],\n",
       "       [ 0.16666667],\n",
       "       [ 0.16666667]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "u_1 = np.asarray([-2,1,1])\n",
    "u_1 = u_1.reshape(3,1)\n",
    "u_1 = u_1/(u_1.T.dot(u_1))\n",
    "u_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28571429],\n",
       "       [-0.42857143],\n",
       "       [-0.14285714]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_2 = np.asarray([0,-2,-1])\n",
    "a_2 = a_2.reshape(3,1)\n",
    "u_2 = a_2 - (u_1.T.dot(a_2)/u_1.T.dot(u_1)) * u_1\n",
    "u_2 = u_2/(u_2.T.dot(u_2))\n",
    "u_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u${3} = [1,1,0]^T - ([1,1,0]^T * \\sqrt(\\frac{2}{7}) * [-1,-3/2,-1/2]^T) * \\sqrt(\\frac{2}{7}) * [-1,-3/2,-1/2]^T - ([1,1,0]^T * \\frac{1}{\\sqrt6} * [-2,1,1]^T) * \\frac{1}{\\sqrt6} * [-2,1,1]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.],\n",
       "       [ 2.],\n",
       "       [-4.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_3 = np.asarray([1,1,0])\n",
    "a_3 = a_3.reshape(3,1)\n",
    "u_3 = a_3 - ((u_1.T.dot(a_3)/u_1.T.dot(u_1)) * u_1) - ((u_2.T.dot(a_3)/u_2.T.dot(u_2)) * u_2)\n",
    "u_3 = u_3/(u_3.T.dot(u_3))\n",
    "u_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.93889390e-18]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_1.T.dot(u_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.66453526e-15]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_1.T.dot(u_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33333333, -0.28571429, -1.        ],\n",
       "       [ 0.16666667, -0.42857143,  2.        ],\n",
       "       [ 0.16666667, -0.14285714, -4.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = [list(u_1.T),list(u_2.T),list(u_3.T)]\n",
    "Q = np.asarray(Q)\n",
    "Q = Q.reshape(3,3)\n",
    "Q = Q.T\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2,  0,  1],\n",
       "       [ 1, -2,  1],\n",
       "       [-1,  1,  0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.asarray([-2,0,1,1,-2,1,-1,1,0])\n",
    "A = A.reshape(3,3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83333333,  0.42857143, -2.        ],\n",
       "       [-0.5       ,  0.42857143, -9.        ],\n",
       "       [ 0.5       , -0.14285714,  3.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.dot(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "\n",
    "Write the Gram-Schmidt algorithm using the code stub below. You may also write your own GS algorithm if you want to pursue more optimized strategies. There are a number of ways that the GS can be written using broadcasting or matrix-based strategies. Tomorrow, we will write the modified GS algorithm that actually ends up being much simpler to implement than the below strategy.\n",
    "\n",
    "Here we actually store the current state of the GS basis in a matrix $Q$ just as if we were writing it by hand. As we build up the GS basis, we iterate through each of the vectors, calculating the projection of the $Q$ vectors $v$ onto each of the input vectors $u$. Then we subtract these projections from the current vector taken from the original set. After normalization, we have our new orthonormal vector that we can add into Q. The algorithm continues on until we have orthonormalized every vector in $A$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def gram_schmidt(A, col_vecs=True, norm=True):\n",
    "    # assumes column vector format of X\n",
    "    \n",
    "    # you may want to check the determinant (why?)\n",
    "    \n",
    "    \n",
    "    # Initialize an empty matrix to store the current state of the GS basis of Q\n",
    "    \n",
    "    # flip the matrix to make the code easier to read\n",
    "    # Easy to process is A is converted into a list\n",
    "\n",
    "    \n",
    "    # * for each vector u in At (the column matrix) for j, col in enumerate(At)\n",
    "    \n",
    "        #np.dot(u,v)\n",
    "        \n",
    "        # * for each of the other vectors v in Q\n",
    "        \n",
    "            # you may need to cast v into the extra dimensions, cast into numpy here:\n",
    "            # v = expand_dims(rowofQ, axis=0)\n",
    "            \n",
    "            # * remove proj_v(u) from u => u-(v.u/v.v)*v\n",
    "            \n",
    "        # * normalize u => np.linalg.norm\n",
    "    \n",
    "        # you'll need to add the new u to your Q matrix => set Q as list, then:\n",
    "        # Q.append(u.flatten())\n",
    "        #Q.append(u.flatten())\n",
    "    \n",
    "    # * return the Q matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def gram_schmidt(A, col_vecs=True, norm=True):\n",
    "    \n",
    "    if(np.linalg.det(A) != 0):\n",
    "    \n",
    "        Q = []\n",
    "    \n",
    "        # Easy to process is A is converted into a list.\n",
    "        # First, we transpose A, so the columns are rows.\n",
    "        A = list(A.T)\n",
    "\n",
    "        # We iterate through rows.\n",
    "        for x in A:\n",
    "            # The current row is a vector.\n",
    "            u = x\n",
    "            # We iterate through Q to calculate the projections and substract them from current row.\n",
    "            for y in Q:\n",
    "                u = u - (u.dot(y)/y.dot(y))* y\n",
    "        \n",
    "            # We normalize the vector.\n",
    "            u = u/np.linalg.norm(u)\n",
    "            # We append the normalized vector to the list.\n",
    "            Q.append(u)\n",
    "\n",
    "        # We return th list Q as an numpy array.\n",
    "        return np.asarray(Q)\n",
    "    else:\n",
    "        print(\"Determinant of\", A, \"is 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.81649658  0.40824829  0.40824829]\n",
      " [-0.53452248 -0.80178373 -0.26726124]\n",
      " [-0.21821789  0.43643578 -0.87287156]]\n",
      "[[ -2.00000000e+00  -1.77509978e-16   1.00000000e+00]\n",
      " [  1.00000000e+00  -2.00000000e+00   1.00000000e+00]\n",
      " [  1.00000000e+00  -1.00000000e+00  -5.90605253e-16]]\n"
     ]
    }
   ],
   "source": [
    "# Quick test harness\n",
    "\n",
    "A = np.array([[-2, 0, 1 ],[1, -2, 1],[1, -1, 0]])\n",
    "Q = gram_schmidt(A)\n",
    "R = Q.dot(A)\n",
    "\n",
    "R = Q.T.dot(A)\n",
    "print(Q)\n",
    "print(Q.dot(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.81649658 -0.53452248 -0.21821789]\n",
      " [ 0.40824829 -0.80178373  0.43643578]\n",
      " [ 0.40824829 -0.26726124 -0.87287156]]\n",
      "[[ -2.00000000e+00   2.67810419e-16   1.00000000e+00]\n",
      " [  1.00000000e+00  -2.00000000e+00   1.00000000e+00]\n",
      " [  1.00000000e+00  -1.00000000e+00  -5.82867088e-16]]\n"
     ]
    }
   ],
   "source": [
    "# Quick test harness\n",
    "\n",
    "A = np.array([[-2, 0, 1 ],[1, -2, 1],[1, -1, 0]])\n",
    "Q = gram_schmidt(A)\n",
    "R = Q.dot(A)\n",
    "\n",
    "R = Q.T.dot(A)\n",
    "print(Q)\n",
    "print(Q.dot(R))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.81649658  0.40824829  0.40824829]\n",
      " [-0.53452248 -0.80178373 -0.26726124]\n",
      " [-0.21821789  0.43643578 -0.87287156]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[-2, 0, 1 ],[1, -2, 1],[1, -1, 0]])\n",
    "\n",
    "gram_schmidt_rows(A.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
