{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6001 5.1 Lecture\n",
    "\n",
    "\n",
    "## The Fundamental Theorem of Linear Algebra and Data Science\n",
    "\n",
    "So far most of what we've learned so far has been delivered to you in terms of relatively atomized topics. To wit we've covered:\n",
    "\n",
    "1. Properties of Vectors\n",
    "2. Properties of Matrices including the four matrix spaces\n",
    "3. Basic operations on Vectors and Matrices\n",
    "4. Linearity, Nonlinearity and solvability\n",
    "5. Special matrices and their properties\n",
    "6. The standard basis and changes of basis\n",
    "7. Orthogonality\n",
    "8. Operations on lines, planes and points\n",
    "9. Matrix decomposition\n",
    "\n",
    "As a whole, this is all you really need to know from linear algebra to proceed with your career. Linear algebraists recognized in the last century that the whole of linear algebra could be summed up in a few relatively simple ideas, and as a capstone on your linear algebra education, we will require that you study the **fundamental theorem of linear algebra** as posed by Gilbert Strang.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The four elements of the Fundamental Theorem:\n",
    "\n",
    "The fundamental theorem of linear algebra contains all of the mathematical elements that we've talked about thus far, listed above as nine bullet points.  It describes the construction of any matrix and how any presumably real-valued matrix can be decomposed into its SVD components by breaking it down into rotations about its subspaces. This is the \n",
    "\n",
    "1. The dimensions of the four subspaces.\n",
    "2. The orthogonality of the four subspaces.\n",
    "3. The basis vectors of the subspaces are orthonormal.\n",
    "4. The matrix with respect to these bases is diagonal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:\n",
    "\n",
    "The below figure describes how $A$ takes members of $\\vec{x}$ in the row space (effectively the domain) into the column space (typically called the codomain) described by $b$. In data science, this can be related directly to taking an array of observations $A$ and mapping them directly to predictions of variable importance $\\vec{x}$. \n",
    "\n",
    "![LA1](./images/LA1_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nullspace (kernel) of A is a space orthogonal to the row space of A. Vectors that are in the nullspace get mapped by A to the 0 on the right-hand side of the diagram. This is also known as the \"left nullspace\" due to that the vectors of the kernel belong to the row space on the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Least squares regression\n",
    "\n",
    "If $b$ is not in the column space, $A\\vec{x} = b$ cannot be solved. Therefore, we have to come up with a \"solution\" for the map. As Strang says, it is far more common than not to have more equations than unknowns (a tall matrix).\n",
    "\n",
    "It is in this case we apply the techniques of regression. We select a definition of the transformation that maps to a combination of the variables:\n",
    "\n",
    "$\\vec{b} = (C+Dt)$\n",
    "\n",
    "or\n",
    "\n",
    "$\\vec{b} = (C+Dt+Et^{2})$\n",
    "\n",
    "that remains linear. \n",
    "\n",
    "However, these are mappings from $\\vec{x}$ to some point $p$ where $p$ is not in the column space. The goal is to minimize the difference between $\\vec{b}$ and $\\vec{p}$, this being the error vector $\\vec{e} = \\vec{b}-\\vec{p}$.\n",
    "\n",
    "The best combination of mappings is $\\tilde{x}$ such that $\\vec{p} = A\\tilde{x}$ is the exact projection of $\\vec{b}$ onto the column space $im(A)$. The vector $\\vec{e}$ is therefore **orthogonal to this projection**. \n",
    "\n",
    "$\\vec{e} = \\vec{b} - A\\tilde{x} = \\vec{b} - proj_{im(A)}b $\n",
    "\n",
    "Because $\\vec{e}$ is orthogonal to $im(A)$, it follows that it is orthogonal with respect to $A^{T}$. This means that the dot product of it with $A^{T}$ is 0:\n",
    "\n",
    "$A^{T}(b-A\\tilde{x}) = 0$\n",
    "\n",
    "$A^{T}b-A^{T}A\\tilde{x} = 0$\n",
    "\n",
    "$A^{T}b = A^{T}A\\tilde{x}$\n",
    "\n",
    "This latter representation is known as the \"normal equation,\" and allows us to find $\\tilde{x}$:\n",
    "\n",
    "$\\tilde{x} = (A^{T}A)^{-1}A^{T}b $\n",
    "\n",
    "$A^{T}A$ is symmetric and real and thus $(A^{T}A)^{-1}$ exists **Given that the columns of $A$ are independent**. Under the use of the normal equations, we make that assumption. Again, $A$ does not have to be invertible, just in possession of independent columns. The below figure describes this operation, particularly the splitting of $\\vec{b}$  into the reachable space $\\vec{p}$ and the unreachable space $\\vec{e}$.\n",
    "\n",
    "![LA2](./images/LA2_small_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Construction of Orthogonal Bases for $A$\n",
    "\n",
    "To complete the theorem, we need to produce bases $v_{1}, \\cdots, v_r \\in V$ for the row space and $u_{1}, \\cdots, u_r \\in U$ for the column space.\n",
    "We can then say that $AV = \\Sigma\\ U$, by definition. $\\Sigma$ is a diagonal matrix in this case, such that  $Av_i = \\sigma_i\\ U_i$. We should also require that $\\sigma_i > 0$.\n",
    "\n",
    "By forcing $V$ to be the (orthogonal) eigenspace of $A^{T}A$, we can write:\n",
    "\n",
    "$A^{T}Av_i = \\sigma_i^{2}\\ v_i$\n",
    "\n",
    "Because $A^{T}A$ and $AA^{T}$ are symmetric, positive semidefinite (and square) they have nonnegative eigenvalues and their eigenvectors in $V$ are orthonormal. This means that the magnitude of $Av_i$ is always equal to the matching eigenvalue.\n",
    "\n",
    "$\\|Av_i\\|=\\sigma_i$\n",
    "\n",
    "Taking the above equation and multiplying again by $A$ from the left, we get:\n",
    "\n",
    "$AA^{T}Av_i = \\sigma_i^{2}A\\ v_i$\n",
    "\n",
    "if you set $u_i = \\frac{1}{\\sigma_i}Av_i$, then you can see that $u_i$ is a unit eigenvector of $AA^{T}$ with eigenvalue $sigma_i$.\n",
    "\n",
    "When you put the whole construction together, you get the SVD:\n",
    "\n",
    "1. $U$, an $m \\times m$ orthogonal matrix. The columns of $U$:$u_{1} \\cdots u_{r} \\cdots u_{m}$ are the basis vectors for the column space and left nullspace. Note that these are orthogonal to each other, so the vectors span both spaces, but they do not overlap (obviously).\n",
    "2. $\\Sigma$, an $m \\times n$ diagonal matrix of eigenvalues \n",
    "3. V is an $n \\times n$ orthogonal matrix. Its columns are the basis vectors for row space and right (the regular) nullspace.\n",
    "\n",
    "\n",
    "This entire construction is depicted in the below figure.\n",
    "\n",
    "![LA3](./images/LA3_small.png)\n",
    "\n",
    "Therefore the SVD expresses A as a combination of $r$ rank-one matrices ( essentially a spectral decomposition):\n",
    "\n",
    "$A = U\\ \\Sigma\\ V^{T} = u_1\\sigma_1\\ v_1^{T}+ \\cdots +u_r\\sigma_r\\ v_r^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Construction of the Full Pseudoinverse\n",
    "\n",
    "The SVD enables us to do something that would otherwise be impossible: We can invert singular matrices using a special construction that recovers the relationship between the left nullspace and the row space.\n",
    "\n",
    "This construction for $A$ takes the column space back to the row space and is called the pseudoinverse, or $A^{+}$. $A^{+} = A^{-1}$ when $det(A) \\neq 0$.\n",
    "\n",
    "The least squares solution of minimum length is by definition: $x^{+} = A^{+}b$. $x^{+} = \\widetilde{x}$ when $A$ has full column rank $r=n$\n",
    "(see the below figure)\n",
    "\n",
    "![LA4](./images/LA4_small.png)\n",
    "\n",
    "In this case, the error vector $e$ described in the second figure is actually the kernel of $A^{+}$, such that $b$ can be thought of as the sum of vectors belonging to $p$ (the projection of $b$ onto the column space) and $e$ the error of this fit. \n",
    "\n",
    "By definition, $A^{+} = V\\ \\Sigma^{+} U^{T}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Summary\n",
    "\n",
    "Summarizing the effects of these different constructions,\n",
    "\n",
    "$Av_{i} = \\sigma_{i}u_{i}$\n",
    "\n",
    "$A^{T}u_{i} = \\sigma_{i}v_{i}$\n",
    "\n",
    "$A^{+}u_{i} = \\frac{1}{\\sigma_{i}}v_{i}$\n",
    "\n",
    "![LA5](./images/LA5_small.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
